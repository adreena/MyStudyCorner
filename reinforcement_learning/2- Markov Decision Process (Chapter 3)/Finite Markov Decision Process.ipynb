{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP\n",
    "![alt bandit](../images/mdp.png)\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "    محیط هایی مثل bandit برای تعریف خیلی از مسایل کافی نیستن و زمانیکه action بر محیط تاثیر داشته باشه  یا state transition  صورت بگیره احتیاج داریم در تصمیم گیری های agent احتمالات بیشتری رو در نظر بگیریم چرن انتخاب یک action همیشه optimal نیست\n",
    "برای حل این نوع از مسایل MDP تعریف میکنیم که اصطلاحا formalizing problem as MDP گفته میشه و شامل \n",
    "\n",
    "- rewards\n",
    "- actions\n",
    "- states\n",
    "</div>\n",
    "    \n",
    "<div dir=\"rtl\">  \n",
    "agent در زمان t با انتخاب action_t در state_t یک reward_t+1 دریافت میکنه و به state_t+1 وارد میشه و محیط وارد مرحله متفاوتی میشه که انتخاب action قبل ممکنه بهترین انتخاب نباشه و احتیاج به محاسبات جدیدی داره تا با کمک سیگنال reward تصمیمات خودش رو update کنه.\n",
    "    برای هر state میتونیم با انتخاب action های مختلف به state های مختلفی وارد بشیم و از اونجا که محیط تصادفی non-stationary هست باید احتمالات رو هم در نظر بگیریم، تعریف MDP در واقع همون state machine هست که برای رفتن به state ها احتمال های مختلفی داریم state-transition probability\n",
    "</div>\n",
    "    \n",
    "<div dir=\"ltr\">  \n",
    "p(s_t+1, r_t+1 | s_t, a_t)\n",
    "</div>\n",
    "    \n",
    "![alt bandit](../images/mdp_example.png)\n",
    "    \n",
    "\n",
    "\n",
    "<div dir=\"rtl\"> در این محیط های sequential و dynamic که با MDP تعریف میشن  یک خصوصیت مشترک داریم به نام Markov property، به این معنی که agent احتیاج به دونستن همه اتفاقات گذشته و تصمیماتی که از آغاز تا الآن داشته نداره و تنها دونستن ۱ time-step قبل کافیه\n",
    "</div>\n",
    "    \n",
    "### Goal and Reward\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "هدف اصلی همچنان جستجو برای دریافت بالاترین expected return در محیط هست، و تنها توجه کردن به immediate reward کافی نیست و باید عواقب هر action رو در طولانی مدت future reward در نظر بگیریم و با کمک gradient update به سمتی حرکت کنیم که مقدار loss کمتر و در نتیجه به optimal policy نزدیک بشیم\n",
    "</div>\n",
    "<div dir=\"rtl\">\n",
    "محیط ها رو میتونیم به ۲ دسته کلی تقسیم کنیم episodic و continuous که تفاوت عمده اونها در داشتن terminal state هست. محیط episodic دارای یک سری independent episode هست که بعد از اتمام هرepisode محیط و agent reset میشن به حالت اولیه. محیط continuous ولی قابلیت شکسته شدن به epiode هارو نداره.\n",
    "ولی برای محیطی که هیچوقت terminate نمیشه چجوری expected future return تعریف میشه؟ این مشکل رو با یک تعریف جدید دیگه به نام discount factor (gamma) حل میکنیم. ایده اصلی اینه که ما با یه ضریبی تاثیر time-step های آینده رو کنترل میکنیم به این صورت که reward های زمان های نزدیکتر مهمتر باشن تا زمان های خیلی دور در آینده به این expected discounted return . میشه اثبات کرد که برای gamma کمتر از ۱ این return متنهایی finite  هست\n",
    "</div>\n",
    "\n",
    "<div dir=\"ltr\">\n",
    "G_t = 1/(1-gamma)\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    " در خیلی از محیط های شبیه سازی شده max-step تعریف میشه که با کمک اون میشه agent رو وادار کرد تصمیمات بهتری در زمان محدود بگیره\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld Random Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3 \t 8.8 \t 4.4 \t 5.3 \t 1.5 \t \n",
      "\n",
      "1.5 \t 3.0 \t 2.3 \t 1.9 \t 0.5 \t \n",
      "\n",
      "0.1 \t 0.7 \t 0.7 \t 0.4 \t -0.4 \t \n",
      "\n",
      "-1.0 \t -0.4 \t -0.4 \t -0.6 \t -1.2 \t \n",
      "\n",
      "-1.9 \t -1.3 \t -1.2 \t -1.4 \t -2.0 \t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(1, '../../reinforcement_learning/code')\n",
    "from env.gridworld_env import GridWorldEnv\n",
    "from matplotlib import pyplot as plt\n",
    "grid_width = 5\n",
    "grid_height = 5\n",
    "env = GridWorldEnv(grid_width, grid_height)\n",
    "action_probability = 0.25\n",
    "discount_factor = 0.9\n",
    "\n",
    "value = np.zeros((grid_width, grid_height))\n",
    "while True:\n",
    "    new_value = np.zeros_like(value)\n",
    "    for i in range(grid_height):\n",
    "        for j in range(grid_width):\n",
    "            for action in env.action_space:\n",
    "                (ni, nj), reward = env.step(state=[i, j], action=action)\n",
    "                new_value[i, j] += action_probability * (reward + discount_factor * value[ni, nj])\n",
    "    \n",
    "    # continue until it reaches tability\n",
    "    diff = np.sum(np.abs(value - new_value))\n",
    "    if diff < 1e-4:\n",
    "        for r in value:\n",
    "            for val in r:\n",
    "                print(\"%.1f \\t\" % round(val, 1), end=\" \")\n",
    "            print('\\n')\n",
    "        break\n",
    "    value = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0 \t 24.4 \t 22.0 \t 19.4 \t 17.5 \t \n",
      "\n",
      "19.8 \t 22.0 \t 19.8 \t 17.8 \t 16.0 \t \n",
      "\n",
      "17.8 \t 19.8 \t 17.8 \t 16.0 \t 14.4 \t \n",
      "\n",
      "16.0 \t 17.8 \t 16.0 \t 14.4 \t 13.0 \t \n",
      "\n",
      "14.4 \t 16.0 \t 14.4 \t 13.0 \t 11.7 \t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_width = 5\n",
    "grid_height = 5\n",
    "env = GridWorldEnv(grid_width, grid_height)\n",
    "action_probability = 0.25\n",
    "discount_factor = 0.9\n",
    "\n",
    "value = np.zeros((grid_width, grid_height))\n",
    "while True:\n",
    "    new_value = np.zeros_like(value)\n",
    "    for i in range(grid_height):\n",
    "        for j in range(grid_width):\n",
    "            action_values = []\n",
    "            for action in env.action_space:\n",
    "                (ni, nj), reward = env.step(state=[i, j], action=action)\n",
    "                action_values.append(reward + discount_factor * value[ni, nj])\n",
    "            new_value[i][j] = np.max(action_values)\n",
    "    # continue until it reaches tability\n",
    "    diff = np.sum(np.abs(value - new_value))\n",
    "    if diff < 1e-4:\n",
    "        for r in value:\n",
    "            for val in r:\n",
    "                print(\"%.1f \\t\" % round(val, 1), end=\" \")\n",
    "            print('\\n')\n",
    "        break\n",
    "    value = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
