{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-armed Bandit\n",
    "---\n",
    "![alt bandit](../images/bandit_intro.png)\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "مساله Bandit یکی از سناریوهاییه که میشه باهاش مفاهیم اولیه Reinforcement Learning رو ساده تر توضیح داد \n",
    "فرض میکنیم یه دستگاه با ۱۰ دسته (environment) داریم ، و ما بعنوان agent هر دفعه با کشیدن یه دسته (action) یک امتیاز  (reward) میگیریم\n",
    "هدف اینه که با چند بار بازی کردن با این environment به حالتی برسیم که بیشترین reward رو جمع کنیم\n",
    "</div><div dir=\"rtl\">  \n",
    "به مساله هایی که انجام دادن یک action تاثیری به environment نداشته باشه یعنی به state دیگه ایی transition انجام نشه Bandit میگن\n",
    "</div><div dir=\"rtl\"> \n",
    "وقتی بازی شروع میشه مقدار reward ایی که با هر action میگیریم رو در یک جدول نگه میداریم و بر اساس اینکه هر action چندبار انتخاب شده و چقدر از بقیه action  ها بهتر هست تصمیم میگیریم که الان کدوم دسته رو بکشیم  به این مرحله تخمین زدن action-value گفته میشه. ولی اگه همیشه به همین روش یعنی به صورت greedy تصمیم بگیریم و مدام از knowledge ایی که جمع کردیم استفاده کنیم (exploit) ممکنه بعضی از action هایی که بهتر  هستن رو هیچوقت کشف (explore) نکنیم ! برای همین راه های متفاوتی برای trade-off بین exploration/exploitation مطرح میشه \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Action Value\n",
    "---\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "برای تخمین action-value از Expection یا میانگین استفاده میکنیم، فرض کنیم زمان t باشه و هربار یه step در environment انجام میدیم. به صورت رایج از (Q_t(a برای نشون دادن ارزش action-a در زمان t استفاده میشه.\n",
    "فرض کنیم در Multi-armed Bandit برای هر action به صورت random از یک normal distribution به agent یک reward داده میشه. حالا اگه با کمک Q و روش greedy شروع به بازی کنیم چه اتفاقی میفته؟\n",
    "</div><div dir=\"rtl\">\n",
    "    البته برای تخمین زدن یکبار run کافی نیست و به صورت رایج چند هزار بار  run ها تکرار و نتایج میانگین گرفته میشه\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../reinforcement_learning/code')\n",
    "from env.bandit_env import BanditEnv\n",
    "from agents.bandit_agent import Agent\n",
    "# TODO implement Multi-armed bandit\n",
    "# display plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "روش مطلقا greedy خیلی زود در انتخاب هاش دچار مشکل میشه ، اصطلاحا در suboptimal action گیر میفته. برای بهتر شدن این روش یه مفهوم جدید تعریف میکنیم با عنوان epsilon-greedy که epsilon در نقش معامله گر بین ۲ حالت exploration و exploitation عمل میکنه. خلاصه مطلب اینه که epsilon از مقداری نزدیک ۱ شروع میشه و طی زمان با کاهش یا decay به مقداری خیلی کمی میرسه و ثابت میشه. agent احتمال epsilon - 1 انتخاب greedy میده و در بقیه موارد به صورت random یکی از action های موجود رو انتخاب میکنه\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
